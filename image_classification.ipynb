{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import necessary modules\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.applications.vgg16 import VGG16\n",
    "# from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.vgg16 import preprocess_input, decode_predictions\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "import cv2\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Saving Bottleneck Features...\n",
      "Found 860 images belonging to 5 classes.\n",
      "18/18 [==============================] - 696s 39s/step\n",
      "Found 160 images belonging to 5 classes.\n",
      "4/4 [==============================] - 145s 36s/step\n",
      "(860, 7, 7, 512)\n",
      "(160, 7, 7, 512)\n"
     ]
    }
   ],
   "source": [
    "from keras.applications import VGG16\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications import imagenet_utils\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Data inputs\n",
    "\n",
    "# Model inputs\n",
    "batch_size = 50\n",
    "epochs = 50\n",
    "top_model_weights_path = 'my_model'\n",
    "\n",
    "image_shape = (224, 224) # VGG16\n",
    "\n",
    "# train_data_dir = '/Users/rmillin/Documents/Insight/animal-tracks/mvpapp/webapp/static/images/train'\n",
    "# test_data_dir = '/Users/rmillin/Documents/Insight/animal-tracks/mvpapp/webapp/static/images/test'\n",
    "train_data_dir = '/Users/rmillin/Documents/Insight/animal-tracks/multiclass/train_grayscale'\n",
    "test_data_dir = '/Users/rmillin/Documents/Insight/animal-tracks/multiclass/test_grayscale'\n",
    "\n",
    "\n",
    "def save_bottleneck_features(image_shape, batch_size, test_data_dir, train_data_dir):\n",
    "    ''' \n",
    "    Saves bottleneck features for testing and training.\n",
    "    '''\n",
    "    print('\\n Saving Bottleneck Features...')\n",
    "\n",
    "\n",
    "    model = VGG16(weights=\"imagenet\", include_top = False)\n",
    "\n",
    "#     datagen = ImageDataGenerator(\n",
    "#         rescale=1./255,\n",
    "#         rotation_range = 10,\n",
    "#         width_shift_range = 0.2,\n",
    "#         height_shift_range = 0.2,\n",
    "#         shear_range = 0,\n",
    "#         zoom_range = 0.2,\n",
    "#         fill_mode = 'nearest'\n",
    "#         )\n",
    "    datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range = 0,\n",
    "        width_shift_range = 0,\n",
    "        height_shift_range = 0,\n",
    "        shear_range = 0,\n",
    "        zoom_range = 0,\n",
    "        fill_mode = 'nearest'\n",
    "        )\n",
    "\n",
    "    train_generator = datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size = image_shape,\n",
    "        batch_size = batch_size,\n",
    "        class_mode = None,\n",
    "        shuffle = False,\n",
    "        save_to_dir='train_images_aug_gray_all', \n",
    "        save_prefix='aug'  # our data will be in order, so all first 1000 images will be cats, then 1000 dogs  # our data will be in order, so all first 1000 images will be cats, then 1000 dogs\n",
    "        )\n",
    "\n",
    "    if not(os.path.isfile('gray_all_bottleneck_features_train.npy')):\n",
    "        train_data = model.predict_generator(train_generator, verbose=1)\n",
    "        np.save(open('gray_all_bottleneck_features_train.npy','wb'), train_data)\n",
    "    else:\n",
    "        train_data = np.load('gray_all_bottleneck_features_train.npy')\n",
    "        \n",
    "    test_generator = datagen.flow_from_directory(\n",
    "        test_data_dir,\n",
    "        target_size = image_shape,\n",
    "        batch_size = batch_size,\n",
    "        class_mode = None,\n",
    "        shuffle = False,\n",
    "        save_to_dir='test_images_aug_gray_all', \n",
    "        save_prefix='aug'  # our data will be in order, so all first 1000 images will be cats, then 1000 dogs  # our data will be in order, so all first 1000 images will be cats, then 1000 dogs\n",
    "        )\n",
    "            \n",
    "    if not(os.path.isfile('gray_all_bottleneck_features_test.npy')):\n",
    "        test_data = model.predict_generator(test_generator, verbose=1)\n",
    "        np.save(open('gray_all_bottleneck_features_test.npy','wb'), test_data)\n",
    "    else:\n",
    "        test_data = np.load('gray_all_bottleneck_features_test.npy')\n",
    "        \n",
    "    # with a Sequential model\n",
    "    layer_name = 'block3_pool'\n",
    "    intermediate_layer_model = Model(inputs=model.input,\n",
    "                                 outputs=model.get_layer(layer_name).output)\n",
    "\n",
    "    train_activations = intermediate_layer_model.predict_generator(train_generator)\n",
    "    test_activations = intermediate_layer_model.predict_generator(test_generator)\n",
    "\n",
    "    \n",
    "    train_y = to_categorical(train_generator.classes)\n",
    "    test_y = to_categorical(test_generator.classes)\n",
    "    train_labels = train_generator.class_indices\n",
    "    np.save(open('train_y.npy','wb'), train_y)\n",
    "    np.save(open('test_y.npy','wb'), test_y)\n",
    "    np.save(open('train_labels.npy','wb'), train_labels)\n",
    "        \n",
    "    return train_data, test_data, train_y, train_labels, test_y, train_activations, test_activations\n",
    "\n",
    "def train_top_model(train_data, train_y, test_data, test_y, n_classes, top_model_weights_path):\n",
    "    ''' \n",
    "    Train top layer with bottleneck features as input\n",
    "    '''\n",
    "\n",
    "    print('\\n Training the FC Layers...')\n",
    "   \n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape = train_data.shape[1:]))\n",
    "    model.add(Dense(2056, activation = 'relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1028, activation = 'relu'))\n",
    "    model.add(Dense(n_classes, activation = 'softmax'))\n",
    "\n",
    "    opt = optimizers.SGD(lr = 1.0e-4, momentum=0.9)\n",
    "    model.compile(optimizer = opt, loss = 'categorical_crossentropy',\n",
    "                 metrics = ['accuracy'])\n",
    "\n",
    "    checkpointer = ModelCheckpoint(filepath='model.best.hdf5', verbose=1, save_best_only=False)\n",
    "   \n",
    "    model.fit(train_data, train_y,\n",
    "             epochs=epochs,\n",
    "             batch_size=batch_size,\n",
    "             validation_data = [test_data, test_y],\n",
    "             callbacks = [checkpointer])\n",
    "\n",
    "    model.save_weights(top_model_weights_path)\n",
    "\n",
    "    return model  \n",
    "\n",
    "if (not os.path.isfile('gray_all_bottleneck_features_train.npy')) or (not os.path.isfile('gray_all_bottleneck_features_test.npy')):\n",
    "    train_data, test_data, train_y, train_labels, test_y, train_activations, test_activations = \\\n",
    "        save_bottleneck_features(image_shape, \\\n",
    "        batch_size, test_data_dir, train_data_dir)\n",
    "else:\n",
    "    train_data = np.load('gray_all_bottleneck_features_train.npy')\n",
    "    test_data = np.load('gray_all_bottleneck_features_test.npy')\n",
    "    train_y = np.load('train_y.npy')\n",
    "    test_y = np.load('test_y.npy') \n",
    "    train_labels = np.load('train_labels.npy')\n",
    "    \n",
    "print(train_data.shape)\n",
    "print(test_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 354 samples, validate on 64 samples\n",
      "Epoch 1/100\n",
      "354/354 [==============================] - 2s 5ms/step - loss: 6.8472 - acc: 0.5169 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 2/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 3/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 4/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 5/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 6/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 7/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 8/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 9/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 10/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 11/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 12/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 13/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 14/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 15/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 16/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 17/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 18/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 19/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 20/100\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 21/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 22/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 23/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 24/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 25/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 26/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 27/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 28/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 29/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 30/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 31/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 32/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 33/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 34/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 35/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 36/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 37/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 38/100\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 39/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 40/100\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 41/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 42/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 43/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 44/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 45/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 46/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 47/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 48/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 49/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 50/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 51/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 52/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 53/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 54/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 55/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 56/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 57/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 58/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 59/100\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 60/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 61/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 63/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 64/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 65/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 66/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 67/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 68/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 69/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 70/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 71/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 72/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 73/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 74/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 75/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 76/100\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 77/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 78/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 79/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 80/100\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 81/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 82/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 83/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 84/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 85/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 86/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 87/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 88/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 89/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 90/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 91/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 92/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 93/100\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 94/100\n",
      "354/354 [==============================] - 1s 4ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 95/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 96/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 97/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 98/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 99/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 100/100\n",
      "354/354 [==============================] - 1s 3ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n"
     ]
    }
   ],
   "source": [
    "train_data = np.load('gray_all_bottleneck_features_train.npy')\n",
    "# the features were saved in order, so recreating the labels is easy\n",
    "train_labels = np.array([0] * int(train_data.shape[0]/2) + [1] * int(train_data.shape[0]/2))\n",
    "# train_labels = np.array([0] * 69 + [1] * 70\n",
    "# train_labels = np.load('train_y.npy')\n",
    "\n",
    "validation_data = np.load('gray_all_bottleneck_features_test.npy')\n",
    "validation_labels = np.array([0] * int(test_data.shape[0]/2) + [1] * int(test_data.shape[0]/2))\n",
    "# validation_labels = np.load('test_y.npy')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=train_data.shape[1:]))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_data, train_labels,\n",
    "          epochs=100,\n",
    "          batch_size=batch_size,\n",
    "          validation_data=(validation_data, validation_labels))\n",
    "model.save_weights('bottleneck_fc_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(328, 28, 28, 256)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_activations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.hist(test_data.flatten())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load('gray_all_bottleneck_features_train.npy')\n",
    "train_data = np.reshape(train_data,(train_data.shape[0], 512*7*7))\n",
    "# train_data = np.reshape(train_activations,(train_activations.shape[0], 28*28*256))\n",
    "# the features were saved in order, so recreating the labels is easy\n",
    "# train_labels = np.load('train_y.npy')\n",
    "# train_labels = train_labels[:,0]\n",
    "# train_labels = np.array([0] * int(train_data.shape[0]/2) + [1] * int(train_data.shape[0]/2))\n",
    "train_labels = np.array([0] * int(train_data.shape[0]/5) + [1] * int(train_data.shape[0]/5) + [2] * int(train_data.shape[0]/5) + [3] * int(train_data.shape[0]/5) + [4] * int(train_data.shape[0]/5))\n",
    "# train_labels = np.array([0] * 276 + [1] * 280)\n",
    "\n",
    "test_data = np.load('gray_all_bottleneck_features_test.npy')\n",
    "test_data = np.reshape(test_data,(test_data.shape[0], 512*7*7))\n",
    "# test_labels = np.load('test_y.npy')\n",
    "# test_labels = test_labels[:,0]\n",
    "# test_data = np.reshape(test_activations,(test_activations.shape[0], 28*28*256))\n",
    "# test_labels = np.array([0] * int(test_data.shape[0]/2) + [1] * int(test_data.shape[0]/2))\n",
    "test_labels = np.array([0] * int(test_data.shape[0]/5) + [1] * int(test_data.shape[0]/5) + [2] * int(test_data.shape[0]/5) + [3] * int(test_data.shape[0]/5) + [4] * int(test_data.shape[0]/5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1720\n",
      "860\n",
      "320\n",
      "160\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(np.sum(train_labels))\n",
    "print(len(train_labels))\n",
    "print(np.sum(test_labels))\n",
    "print(len(test_labels))\n",
    "\n",
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda/envs/insight/lib/python3.6/site-packages/sklearn/preprocessing/data.py:653: RuntimeWarning: invalid value encountered in sqrt\n",
      "  self.scale_ = _handle_zeros_in_scale(np.sqrt(self.var_))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 2 2 2 0 2 0 0 3 0 4 0 0 4 2 1 2 2 2 1 4 4 0 3 0 0 0 0 2 2 2 2 0 1 1 1 3\n",
      " 1 2 1 4 4 0 1 0 1 0 4 1 1 4 1 3 1 1 4 4 1 1 1 1 3 4 4 2 1 4 1 2 2 4 2 2 2\n",
      " 0 0 3 3 1 1 1 1 1 1 1 2 2 2 3 4 3 2 0 4 4 4 4 2 2 2 1 1 3 1 3 2 0 0 2 3 3\n",
      " 3 2 2 2 2 3 3 3 3 3 3 3 0 3 4 3 3 1 2 0 1 4 4 4 1 2 4 4 1 2 4 4 4 3 0 3 3\n",
      " 4 0 2 4 1 1 3 4 4 1 4 4]\n",
      "Accuracy of Logistic regression classifier on training set: 1.00\n",
      "Accuracy of Logistic regression classifier on test set: 0.41\n"
     ]
    }
   ],
   "source": [
    "# logistic regression classifier on features\n",
    "\n",
    "import os\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# train_data = train_activations\n",
    "# test_data = test_activations\n",
    "\n",
    "s_scaler = preprocessing.StandardScaler()\n",
    "train_data = s_scaler.fit_transform(train_data)\n",
    "test_data = s_scaler.transform(test_data)\n",
    "\n",
    "\n",
    "clf = LogisticRegression(penalty='l1',C=1).fit(train_data, train_labels)\n",
    "pred = clf.predict(test_data)\n",
    "\n",
    "print(pred)\n",
    "np.save(open('pred_nn_raw.npy','wb'), pred)\n",
    "np.save(open('act_nn_raw.npy','wb'), test_labels)\n",
    "\n",
    "print('Accuracy of Logistic regression classifier on training set: {:.2f}'\n",
    "     .format(clf.score(train_data, train_labels)))\n",
    "print('Accuracy of Logistic regression classifier on test set: {:.2f}'\n",
    "     .format(clf.score(test_data, test_labels)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# clf = SVC(C=0.00001,kernel='linear').fit(train_data, train_labels)\n",
    "pred = clf.predict(test_data)\n",
    "\n",
    "# print('Accuracy of SVM classifier on training set: {:.2f}'\n",
    "#      .format(clf.score(train_data, train_labels)))\n",
    "# print('Accuracy of SVM classifier on test set: {:.2f}'\n",
    "#      .format(clf.score(test_data, test_labels)))\n",
    "\n",
    "# train_acc_nn = []\n",
    "# test_acc_nn = []\n",
    "# param_range = [0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000, 100000]\n",
    "# # param_range = [10**(-20), 10**(-15), 10**(-10), 0.000000001, 0.00000001, 0.0000001, 0.000001, 0.00001, 0.0001, 0.001]\n",
    "# for this_C in param_range:\n",
    "#     clf = LogisticRegression(penalty='l1', C=this_C).fit(train_data, train_labels)\n",
    "# #     clf = SVC(C=this_C).fit(train_data, train_labels)\n",
    "#     train_acc_nn = np.append(train_acc_nn,clf.score(train_data,train_labels))\n",
    "#     test_acc_nn = np.append(test_acc_nn, clf.score(test_data,test_labels))\n",
    "\n",
    "\n",
    "# plt.figure()\n",
    "\n",
    "# plt.title('Validation Curve with CNN Features')\n",
    "# plt.xlabel('C')\n",
    "# plt.ylabel('Score')\n",
    "# plt.ylim(0.0, 1.1)\n",
    "# lw = 2\n",
    "\n",
    "# plt.semilogx(param_range, train_acc_nn, label='Training score')\n",
    "\n",
    "# plt.semilogx(param_range, test_acc_nn, label='Testing score')\n",
    "\n",
    "# #plt.show()    \n",
    "# plt.savefig('NN_validation.png')    \n",
    "    \n",
    "    \n",
    "#import pdb    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "plt.title('Validation Curve with CNN Features')\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('Score')\n",
    "plt.ylim(0.0, 1.1)\n",
    "lw = 2\n",
    "\n",
    "plt.semilogx(param_range, train_acc_nn, label='Training score')\n",
    "\n",
    "plt.semilogx(param_range, test_acc_nn, label='Testing score')\n",
    "\n",
    "#plt.show()    \n",
    "plt.savefig('NN_validation.png')    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".DS_Store\n",
      ".DS_Store\n",
      ".DS_Store\n",
      ".DS_Store\n",
      ".DS_Store\n",
      ".DS_Store\n",
      ".DS_Store\n",
      ".DS_Store\n",
      ".DS_Store\n",
      ".DS_Store\n"
     ]
    }
   ],
   "source": [
    "import pdb\n",
    "import cv2\n",
    "\n",
    "\n",
    "animals = ['bear','canine','feline','hooved','others']\n",
    "    \n",
    "# also get accuracy for RGB values\n",
    "training_data=np.empty([0, 224**2])\n",
    "testing_data=np.empty([0, 224**2])\n",
    "for animal in animals:\n",
    "    for name in os.listdir(os.path.join(train_data_dir,animal)):\n",
    "        try:\n",
    "            img = cv2.imread(os.path.join(train_data_dir,animal,name))\n",
    "            training_data = np.append(training_data,np.matrix(img[:,:,0].flatten()),axis=0)\n",
    "        except:\n",
    "#            pdb.set_trace()\n",
    "            print(name)\n",
    "\n",
    "    for name in os.listdir(os.path.join(test_data_dir,animal)):\n",
    "        try:\n",
    "            img = cv2.imread(os.path.join(test_data_dir,animal,name))\n",
    "            testing_data = np.append(testing_data,np.matrix(img[:,:,0].flatten()),axis=0)\n",
    "        except:\n",
    "    #        pdb.set_trace()\n",
    "            print(name)\n",
    "\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "training_data = min_max_scaler.fit_transform(training_data)\n",
    "testing_data = min_max_scaler.transform(testing_data)\n",
    "\n",
    "# train_acc_rgb = []\n",
    "# test_acc_rgb = []\n",
    "# for this_C in param_range:\n",
    "#     clf = LogisticRegression(penalty='l1', C=this_C).fit(training_data, train_labels)\n",
    "# #     clf = SVC(C=this_C).fit(training_data, training_labels)\n",
    "#     train_acc_rgb = np.append(train_acc_rgb,clf.score(training_data,training_labels))\n",
    "#     test_acc_rgb = np.append(test_acc_rgb,clf.score(testing_data,test_labels))\n",
    "\n",
    "\n",
    "# plt.figure()\n",
    "\n",
    "# plt.title('Validation Curve with RGB Values')\n",
    "# plt.xlabel('C')\n",
    "# plt.ylabel('Score')\n",
    "# plt.ylim(0.0, 1.1)\n",
    "# lw = 2\n",
    "\n",
    "# plt.semilogx(param_range, train_acc_rgb, label='Training score')\n",
    "\n",
    "# plt.semilogx(param_range, test_acc_rgb, label='Testing score')\n",
    "\n",
    "# #plt.show()    \n",
    "# plt.savefig('RGB_validation.png')  \n",
    "\n",
    "# print(np.sum(training_labels))\n",
    "# print(len(training_labels))\n",
    "# print(np.sum(testing_labels))\n",
    "# print(len(testing_labels))\n",
    "\n",
    "# np.save(open('pred_intens_filt.npy','wb'), pred)\n",
    "# np.save(open('act_intens_filt.npy','wb'), test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(860, 50176)\n",
      "[0 0 0 0 1 0 1 1 3 3 3 3 1 1 1 1 1 1 1 1 3 3 3 3 2 2 2 2 1 1 1 1 2 2 2 2 3\n",
      " 1 1 1 3 3 3 3 1 1 1 1 3 3 3 3 2 2 2 2 1 2 2 2 1 1 1 1 2 2 2 2 4 4 4 4 0 0\n",
      " 0 0 1 4 1 1 4 4 4 4 1 1 1 1 4 4 4 4 4 4 4 4 3 3 3 3 1 1 1 1 0 0 4 4 1 1 1\n",
      " 1 2 2 3 2 3 3 3 3 2 4 2 2 2 2 2 2 3 1 3 1 0 3 0 1 0 0 0 0 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 3 1 3 1 3 3 3 3]\n",
      "Accuracy of Logistic regression classifier on training set: 0.95\n",
      "Accuracy of Logistic regression classifier on test set: 0.26\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(training_data.shape)\n",
    "clf = LogisticRegression(penalty='l1',C=1).fit(training_data, train_labels)\n",
    "pred = clf.predict(testing_data)\n",
    "\n",
    "print(pred)\n",
    "np.save(open('pred_intens_raw.npy','wb'), pred)\n",
    "np.save(open('act_intens_raw.npy','wb'), test_labels)\n",
    "\n",
    "print('Accuracy of Logistic regression classifier on training set: {:.2f}'\n",
    "     .format(clf.score(training_data, train_labels)))\n",
    "print('Accuracy of Logistic regression classifier on test set: {:.2f}'\n",
    "     .format(clf.score(testing_data, test_labels)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras import applications\n",
    "\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 50, 50\n",
    "\n",
    "top_model_weights_path = 'bottleneck_fc_model.h5'\n",
    "train_data_dir = '/Users/rmillin/Documents/Insight/animal-tracks/mvpapp/webapp/static/images/train'\n",
    "validation_data_dir = '/Users/rmillin/Documents/Insight/animal-tracks/mvpapp/webapp/static/images/test'\n",
    "nb_train_samples = 2000\n",
    "nb_validation_samples = 800\n",
    "batch_size = 16\n",
    "\n",
    "\n",
    "def save_bottleneck_features():\n",
    "    datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "    # build the VGG16 network\n",
    "    model = applications.VGG16(include_top=False, weights='imagenet', input_shape=(50,50,3))\n",
    "\n",
    "    generator = datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "    bottleneck_features_train = model.predict_generator(\n",
    "        generator, nb_train_samples // batch_size)\n",
    "    np.save('bottleneck_features_train.npy',\n",
    "            bottleneck_features_train)\n",
    "\n",
    "    generator = datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "    bottleneck_features_validation = model.predict_generator(\n",
    "        generator, nb_validation_samples // batch_size)\n",
    "    np.save('bottleneck_features_validation.npy',\n",
    "            bottleneck_features_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 106 images belonging to 2 classes.\n",
      "Found 20 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "save_bottleneck_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(92,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'layer_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-ebbac4743027>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# get output from layer layer_name = 'my_layer'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m intermediate_layer_model = Model(inputs=base_model.input,\n\u001b[0;32m----> 3\u001b[0;31m                                  outputs=base_model.get_layer(layer_name).output)\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mintermediate_layer_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'layer_name' is not defined"
     ]
    }
   ],
   "source": [
    "# get output from layer layer_name = 'my_layer'\n",
    "intermediate_layer_model = Model(inputs=base_model.input,\n",
    "                                 outputs=base_model.get_layer(layer_name).output)\n",
    "intermediate_output = intermediate_layer_model.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([name for name in os.listdir('/Users/rmillin/Documents/Insight/animal-tracks/images/first_test/test/dog')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44, 512)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7500"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "50*50*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/rmillin/Documents/Insight/web-app/flaskexample/static/images/train/cougar/.DS_Store'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(train_dir,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 44 images belonging to 2 classes.\n",
      "train\n",
      "Found 23 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "#     preprocessing_function=preprocess_input,\n",
    "\n",
    "train_datagen =  image.ImageDataGenerator(\n",
    "    rotation_range=90,\n",
    "    shear_range=0,\n",
    "    zoom_range=0,\n",
    "    vertical_flip=True,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "#     preprocessing_function=preprocess_input,\n",
    "\n",
    "test_datagen = image.ImageDataGenerator(\n",
    "    rotation_range=0,\n",
    "    shear_range=0,\n",
    "    zoom_range=0,\n",
    "    vertical_flip=False,\n",
    "    horizontal_flip=False\n",
    ")\n",
    "#pdb.set_trace()\n",
    "batch_size_train = 16\n",
    "batch_size_test = 1\n",
    "# os.makedirs('train_images_aug')\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "  '/Users/rmillin/Documents/Insight/animal-tracks/mvpapp/webapp/static/images/train',\n",
    "  target_size=(50,50),\n",
    "  batch_size=batch_size_train,\n",
    "  class_mode=None,  # this means our generator will only yield batches of data, no labels\n",
    "  shuffle=False,\n",
    "  save_to_dir='train_images_aug', \n",
    "  save_prefix='aug')  # our data will be in order, so all first 1000 images will be cats, then 1000 dogs  # our data will be in order, so all first 1000 images will be cats, then 1000 dogs\n",
    "for k in train_generator:\n",
    "    print('train')\n",
    "    break\n",
    "# os.makedirs('test_images_aug')\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "  '/Users/rmillin/Documents/Insight/animal-tracks/mvpapp/webapp/static/images/test',\n",
    "  target_size=(50,50),\n",
    "  batch_size=batch_size_test,\n",
    "  class_mode=None,  # this means our generator will only yield batches of data, no labels\n",
    "  shuffle=False,\n",
    "  save_to_dir='test_images_aug', \n",
    "  save_prefix='aug')  # our data will be in order, so all first 1000 images will be cats, then 1000 dogs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 92 images belonging to 2 classes.\n",
      "Found 20 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras import applications\n",
    "\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 50, 50\n",
    "\n",
    "top_model_weights_path = 'bottleneck_fc_model.h5'\n",
    "train_data_dir = '/Users/rmillin/Documents/Insight/animal-tracks/mvpapp/webapp/static/images/train'\n",
    "validation_data_dir = '/Users/rmillin/Documents/Insight/animal-tracks/mvpapp/webapp/static/images/test'\n",
    "nb_train_samples = 92\n",
    "nb_validation_samples = 20\n",
    "batch_size = 2\n",
    "\n",
    "\n",
    "def save_bottlebeck_features():\n",
    "    datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "    # build the VGG16 network\n",
    "#    model = applications.VGG16(include_top=False, weights='imagenet', pooling='max')\n",
    "    model = applications.VGG16(include_top=False, weights='imagenet')\n",
    "\n",
    "    generator = datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        class_mode=None,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False)\n",
    "    bottleneck_features_train = model.predict_generator(\n",
    "        generator, nb_train_samples // batch_size)\n",
    "    np.save('bottleneck_features_train.npy',\n",
    "            bottleneck_features_train)\n",
    "\n",
    "    generator = datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        class_mode=None,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False)\n",
    "    bottleneck_features_validation = model.predict_generator(\n",
    "        generator, nb_validation_samples // batch_size)\n",
    "    np.save('bottleneck_features_validation.npy',\n",
    "            bottleneck_features_validation)\n",
    "\n",
    "save_bottlebeck_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6190476190476191"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "13/21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the bottleneck features to use for classification\n",
    "\n",
    "vgg_conv = VGG19(weights='imagenet',\n",
    "                  include_top=False,\n",
    "                  input_shape=(50, 50,3))\n",
    "\n",
    "bottleneck_features_train = vgg_conv.predict_generator(train_generator)\n",
    "# save the output as a Numpy array\n",
    "np.save('bottleneck_features_train.npy', bottleneck_features_train)\n",
    "\n",
    "bottleneck_features_test = vgg_conv.predict_generator(test_generator)\n",
    "# save the output as a Numpy array\n",
    "np.save('bottleneck_features_test.npy', bottleneck_features_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of each class in train and test\n",
    "import fnmatch\n",
    "import os\n",
    "\n",
    "# -1 below for DSstore file.  NEED TO CHECK AND FIX!!!!\n",
    "n_dogs = 22\n",
    "n_cougars = 22\n",
    "train_labels = np.array([0] * n_cougars + [1] * n_dogs)\n",
    "\n",
    "n_dogs = 8\n",
    "n_cougars = 15\n",
    "# n_dogs = len([name for name in os.listdir('/Users/rmillin/Documents/Insight/animal-tracks/images/first_test/test/dog')])-1\n",
    "# n_cougars = len([name for name in os.listdir('/Users/rmillin/Documents/Insight/animal-tracks/images/first_test/test/cougar')])-1\n",
    "test_labels = np.array([0] * n_cougars + [1] * n_dogs)\n",
    "\n",
    "\n",
    "# load the data into numpy arrays\n",
    "train_data = np.squeeze(np.load('bottleneck_features_train.npy'))\n",
    "# the features were saved in order, so recreating the labels is easy\n",
    "\n",
    "test_data = np.squeeze(np.load('bottleneck_features_test.npy'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.training.Model at 0x1a5f82a080>"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = VGG16(weights=\"imagenet\", include_top = False)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'block3_pool'"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[10].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
